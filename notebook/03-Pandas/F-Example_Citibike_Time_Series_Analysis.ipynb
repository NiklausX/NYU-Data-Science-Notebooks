{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Analyzing Citibike Station Activity using Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import matplotlib \n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.style.use(['seaborn-talk', 'seaborn-ticks', 'seaborn-whitegrid'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use the database of snapshots of Citibike stations statuses. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first fetch the data from the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "\n",
    "conn_string = 'mysql://{user}:{password}@{host}/?charset=utf8mb4'.format(\n",
    "    host = 'db.ipeirotis.org', \n",
    "    user = 'student',\n",
    "    password = 'dwdstudent2015',\n",
    "    encoding = 'utf8mb4')\n",
    "engine = create_engine(conn_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT * \n",
    "FROM citibike.stations\n",
    "'''\n",
    "df = pd.read_sql(query, con=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Analysis\n",
    "\n",
    "As a first step, let's see how the status of the bike stations evolves over time. We compute the average \"fullness\" of all the bike stations over time. We can use the `groupby` function of pandas, and compute the `mean()` for the groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice that this also returns an average for the station ID's which is kind of useless\n",
    "# We will eliminate these next.\n",
    "df.groupby('communication_time').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot the activity over time. We can see that the percentage of bikes in the stations falls from 35% overnight to 30% during the morning and evening commute times, while the average availability during the day is around 31%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('communication_time').mean().percent_full.plot(\n",
    "    figsize=(20,10), grid=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do also the seasonal decomposition to see the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "time_series = df.groupby('communication_time').mean().percent_full\n",
    "\n",
    "# We decompose assumming a 24-hour periodicity. \n",
    "# There is a weekly component as well, which we ignore.\n",
    "# We can also specify a multiplicative instead of an additive model\n",
    "# The additive model is Y[t] = T[t] + S[t] + e[t]\n",
    "# The multiplicative model is Y[t] = T[t] * S[t] * e[t]\n",
    "decomposition = seasonal_decompose(time_series, model='multiplicative', freq=24)\n",
    "\n",
    "seasonal = decomposition.plot()  \n",
    "seasonal.set_size_inches(20, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examining Time Series per Station\n",
    "\n",
    "We now create a pivot table, to examine the time series for individual stations.\n",
    "\n",
    "Notice that we use the `fillna` method, where we fill the cells where we do not have values using the prior, non-missing value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "df2 = df.pivot_table(\n",
    "    index='communication_time', \n",
    "    columns='id', \n",
    "    values='percent_full', \n",
    "    aggfunc=np.mean\n",
    ").interpolate(method='time') \n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the time series for *all* bike stations, for a couple of days in February."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.plot(\n",
    "    alpha=0.05, \n",
    "    color='b', \n",
    "    legend=False, \n",
    "    figsize=(20,10), \n",
    "    xlim=('2017-02-15','2017-02-19')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's limit our plot to just two stations:\n",
    "* Station 3260 at \"Mercer St & Bleecker St\"\n",
    "* Station 161 at \"LaGuardia Pl & W 3 St\"\n",
    "\n",
    "which are nearby and tend to exhibit similar behavior. Remember that the list of stations is [available as a JSON](https://feeds.citibikenyc.com/stations/stations.json) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2[[161, 3260]].plot(\n",
    "    alpha=0.5,  \n",
    "    legend=False, \n",
    "    figsize=(20,10), \n",
    "   xlim=('2017-02-15','2017-02-27')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Bike Stations with Similar Behavior\n",
    "\n",
    "For our next analysis, we are going to try to find bike stations that have similar behaviors over time. A very simple technique that we can use to find similar time series is to treat the time series as vectors, and compute their correlation. Pandas provides the `corr` function that can be used to calculate the correlation of columns. (If we want to compute the correlation of rows, we can just take the transpose of the dataframe using the `transpose()` function, and compute the correlations there.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities = df2.corr(method='pearson')\n",
    "similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the similarities of the two stations that we examined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations = [161, 3260]\n",
    "\n",
    "similarities[stations].loc[stations]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For bookkeeping purposes, we are going to drop rows that contain NaN values, as we cannot use such similarity values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities.dropna(axis='index', how='any', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to convert our similarities into distance metrics, that are positive, and bounded to be between 0 and 1.\n",
    "\n",
    "* If two stations have correlation 1, they behave identically, and therefore have distance 0, \n",
    "* If two stations have correlation -1, they have exactly the oppositite behaviors, and therefore we want to have distance 1 (the max) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = ((.5*(1-similarities))**2)\n",
    "distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering Based on Distances\n",
    "\n",
    "Without explaining too much about clustering, we are going to use a clustering technique and cluster together bike stations that are \"nearby\" according to our similarity analysis. The code is very simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "cluster = KMeans(n_clusters=2)\n",
    "cluster.fit(distances.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now take the results of the clustering and associate each of the data points into a cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.DataFrame(list(zip(distances.index.values.tolist(), cluster.labels_)), columns = [\"id\", \"cluster\"])\n",
    "labels.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how many stations in each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.groupby('cluster').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Time Series Clusters\n",
    "\n",
    "We will start by assining a color to each cluster, so that we can plot each station-timeline with the cluster color. (We put a long list of colors, so that we can play with the number of clusters in the earlier code, and still get nicely colored results.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = list(['r','b', 'g', 'm', 'y', 'k', 'w', 'c'])\n",
    "labels['color'] = labels['cluster'].apply(lambda cluster_id : colors[cluster_id]) \n",
    "labels.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_plot = df2.plot(\n",
    "    alpha=0.02, \n",
    "    legend=False, \n",
    "    figsize=(20,5), \n",
    "    color=labels[\"color\"],\n",
    "    #xlim=('2017-02-15','2017-02-17')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot still looks messy. Let's try to plot instead a single line for each cluster. To represent the cluster, we are going to use the _median_ fullness value across all stations that belong to a cluster, for each timestamp. For that, we can again use a pivot table: we define the `timestamp` as one dimension of the table, and `cluster` as the other dimension, and we use the `percentile` function to compute the median. \n",
    "\n",
    "For that, we first _join_ our original dataframe with the results of the clustering, using the `merge` command, and add an extra column that includes the clusterid for each station. Then we compute the pivot table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "median_cluster = df.merge(\n",
    "    labels, \n",
    "    how='inner', \n",
    "    on='id'\n",
    ").pivot_table(\n",
    "    index='communication_time', \n",
    "    columns='cluster', \n",
    "    values='percent_full', \n",
    "    aggfunc=lambda x: np.percentile(x, 50) # median\n",
    ")\n",
    "\n",
    "median_cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can plot the medians for the two clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_plot = median_cluster.plot(\n",
    "        figsize=(20,5), \n",
    "        linewidth = 2, \n",
    "        alpha = 0.75,\n",
    "        color=colors,\n",
    "        ylim = (0,0.75),\n",
    "        xlim=('2017-02-13','2017-02-20'),\n",
    "        grid = True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And just for fun and for visual decoration, let's put the two plots together. We are going to fade a lot the individual station time series (by putting the `alpha=0.01`) and we are going to make more prominent the median lines by increasing their linewidths. We will limit our plot to one week's worth of data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_plot = df2.plot(\n",
    "    alpha=0.01, \n",
    "    legend=False, \n",
    "    figsize=(20,5), \n",
    "    color=labels[\"color\"]\n",
    ")\n",
    "\n",
    "median_cluster.plot(\n",
    "    figsize=(20,5), \n",
    "    linewidth = 3, \n",
    "    alpha = 0.5,\n",
    "    color=colors, \n",
    "    xlim=('2017-02-13','2017-02-20'),\n",
    "    ax = stations_plot\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
